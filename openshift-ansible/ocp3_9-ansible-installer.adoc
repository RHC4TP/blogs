== Installing OpenShift Container Platform 3.9 using Ansible

OpenShift Container Platform 3.9 was https://blog.openshift.com/announcing-the-openshift-container-platform-3-9-ga/[recently released], jumping right past 3.8 in the process. This wasn't a version number inflation, but rather a double rebase, meaning that code for 3.8 was merged and tested before repeating the process for 3.9 (only the release was skipped). The full release notes and list of new features for OCP 3.9 can be found https://docs.openshift.com/container-platform/3.9/release_notes/ocp_3_9_release_notes.html[here]. One thing to note, is that the Quick Installation has been deprecated in favor of using Ansible playbooks (still known as the Advanced Installation).

Thats right... the Quick Installation is now deprecated and will be removed in a future release. Cluster hosts and installation options will now need to be defined in an Ansible hosts file, instead of being prompted for at install time. Despite this change, the underlying installation method (Ansible) is still the same. Ansible playbooks were always used to install OpenShift, the only difference being that the Quick Installation created the hosts file for you, and then called `ansible-playbook` against the generated file.

In this post, you'll be guided through the installation process, where it is assumed that at least 3 hosts (1 master and 2 nodes) meeting the https://docs.openshift.com/container-platform/3.9/install_config/install/prerequisites.html[minimum requirements] are https://docs.openshift.com/container-platform/3.9/install_config/install/host_preparation.html[prepared] to install OpenShift.
The installation process involves modifying an example Ansible hosts file, copying it into place, and then calling `ansible-playbook`. 

You can obtain the example hosts file for OpenShift https://raw.githubusercontent.com/openshift/openshift-ansible/master/inventory/hosts.example[here], or alternatively by copying it from the following location on the OCP installation host, which is assumed to be the master host (from where all example commands are run as `root`):

----
# cp /usr/share/doc/openshift-ansible-docs-3.9.14/docs/example-inventories/hosts.example ./
----

Below is a snippet from the uppermost section of the `hosts.example` file as it gets shipped in OCP 3.9, along with notations explaining each section:

----
[masters] <1>
ose3-master[1:3].test.example.com

[etcd]
ose3-master[1:3].test.example.com <2>

[nodes]
ose3-master[1:3].test.example.com <3>
ose3-infra[1:2].test.example.com openshift_node_labels="{'region': 'infra', 'zone': 'default'}"
ose3-node[1:2].test.example.com openshift_node_labels="{'region': 'primary', 'zone': 'default'}"

[nfs]
ose3-master1.test.example.com

[lb]
ose3-lb.test.example.com

# Create an OSEv3 group that contains the masters and nodes groups
[OSEv3:children] <4>
masters
nodes
etcd
lb
nfs

[OSEv3:vars] <5> <6>
###############################################################################
# Common/ Required configuration variables follow                             #
###############################################################################
# SSH user, this user should allow ssh based auth without requiring a
# password. If using ssh key based auth, then the key should be managed by an
# ssh agent.
ansible_user=root

# If ansible_user is not root, ansible_become must be set to true and the
# user must be configured for passwordless sudo
#ansible_become=yes

# Specify the deployment type. Valid values are origin and openshift-enterprise.
openshift_deployment_type=origin
#openshift_deployment_type=openshift-enterprise

# Specify the generic release of OpenShift to install. This is used mainly just during installation, after which we
# rely on the version running on the first master. Works best for containerized installs where we can usually
# use this to lookup the latest exact version of the container images, which is the tag actually used to configure
# the cluster. For RPM installations we just verify the version detected in your configured repos matches this
# release.
openshift_release=v3.9

# default subdomain to use for exposed routes, you should have wildcard dns
# for *.apps.test.example.com that points at your infra nodes which will run
# your router
openshift_master_default_subdomain=apps.test.example.com

#Set cluster_hostname to point at your load balancer
openshift_master_cluster_hostname=ose3-lb.test.example.com


<7>
###############################################################################
# Additional configuration variables follow                                   #
###############################################################################
...
----
<1> The ini config file format is used for the Ansible hosts file, with host groups defined in square brackets `[ ]`
<2> The master hosts are listed in multiple groups, since they are typically used in additional roles (such as etcd and nfs)
<3> Sequence matching such as `[1..3]` is used to list multiple hosts onto the same line
<4> `[OSEv3:children]` lists the subgroups that will be configured during the installation
<5> `[OSEv3:vars]` is where all of the cluster variables (installation options) are defined
<6> This upper section of `[OSEv3:vars]` contains key variables that must be changed according to your environment
<7> Variables listed below this point are considered optional (most are commented out), but are included for fine-tunability

All of these example hostnames will need to be changed to match the hostnames within your cluster. In this walkthrough, we use an example environment of 1 master and 2 nodes, and an example domain name of `openshift.local`. Below, you'll see the `hosts.example` file modified to reflect the hostnames and variables used in this example, with each specific change notated:

----
[masters]
master1.openshift.local ansible_connection=local <1>

[etcd]
master1.openshift.local ansible_connection=local

[nodes]
master1.openshift.local ansible_connection=local
node1.openshift.local openshift_node_labels="{'region': 'infra', 'zone': 'default'}" <2>
node2.openshift.local openshift_node_labels="{'region': 'primary', 'zone': 'default'}" <3>

[nfs]
master1.openshift.local ansible_connection=local

# Create an OSEv3 group that contains the masters and nodes groups
[OSEv3:children] <4>
masters
nodes
etcd
nfs

[OSEv3:vars]
###############################################################################
# Common/ Required configuration variables follow                             #
###############################################################################

# SSH user, this user should allow ssh based auth without requiring a
# password. If using ssh key based auth, then the key should be managed by an
# ssh agent.
ansible_user=root <5>

# If ansible_user is not root, ansible_become must be set to true and the
# user must be configured for passwordless sudo
#ansible_become=yes

# Specify the deployment type. Valid values are origin and openshift-enterprise.
#openshift_deployment_type=origin
openshift_deployment_type=openshift-enterprise <6>

# Specify the generic release of OpenShift to install. This is used mainly just during installation, after which we
# rely on the version running on the first master. Works best for containerized installs where we can usually
# use this to lookup the latest exact version of the container images, which is the tag actually used to configure
# the cluster. For RPM installations we just verify the version detected in your configured repos matches this
# release.
openshift_release=v3.9

# default subdomain to use for exposed routes, you should have wildcard dns
# for *.apps.test.example.com that points at your infra nodes which will run
# your router
#openshift_master_default_subdomain=apps.test.example.com <7>

#Set cluster_hostname to point at your load balancer
#openshift_master_cluster_hostname=ose3-lb.test.example.com <8>



###############################################################################
# Additional configuration variables follow                                   #
###############################################################################
...
----
<1> `master1.openshift.local` is configured as a `[master]`, `[node]`, `[etcd]` and `[nfs]` host, while `ansible_connection=local` is set since we will install from this host
<2> `node1.openshift.local` will act as the infrastructure node (to host the registry, router and service brokers), denoted by `region: infra`
<3> `node2.openshift.local` will be our primary node for hosting applications, denoted by `region: primary`
<4> A load balancer is not utilized, so the `[lb]` group was removed, as well as the corresponding entry from `[OSEv3:children]`
<5> If you don't use the `root` account for passwordless access over ssh, then set `ansible_user` to the desired username (also requires passwordless sudo permissions)
<6> To install OpenShift Container Platform (and not OpenShift Origin), you must comment out `openshift_deployment_type=origin` and uncomment `openshift_deployment_type=openshift-enterprise`
<7> If you don't have wildcard DNS setup for your cluster, then you can safely comment this out (defaulting to the hostname of the primary master)
<8> We comment this line out since a load balancer isn't used (once again defaulting to the hostname of the primary master)

If you are deploying to an environment with limited resources (such as a laptop) for development or testing purposes, then you must disable the memory and disk availability checks that occur during the install. You can do this by adding the following line anywhere beneath the `[OSEv3:vars]` section:

----
openshift_disable_check=memory_availability,disk_availability
----

[NOTE]
====
You can further condense the installation down to 2 hosts by eliminating the infrastructure node (denoted by `region: infra` in the `hosts.example` file).

Be aware that this will cause the Template Broker deployment to fail, which happens to be the very last phase of the install.
====

There is one final tweak that must be made to the Ansible hosts file before copying it into place. Scroll down into the `[OSEv3:vars]` section and uncomment the following line to enable htpasswd authentication as shown: 

----
# htpasswd auth
openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider', 'filename': '/etc/origin/master/htpasswd'}]
----

Once you've finished editing the `hosts.example` file, you can move it into place, backing up the original `/etc/ansible/hosts` file beforehand. Assuming you are in the current directory of your modified `hosts.example` file, run the following commands:

----
# mv /etc/ansible/hosts{,.orig}
# mv hosts.example /etc/ansible/hosts
----

The Ansible hosts file is now in place and ready for use. The next command will launch Ansible, and install OpenShift 3.9 using the official playbook set:

----
# ansible-playbook /usr/share/ansible/openshift-ansible/playbooks/deploy_cluster.yml
----

Now, crack open a cold beverage (or go grab some coffee if you prefer) and wait for the installation to complete. Assuming that each host was properly https://docs.openshift.com/container-platform/3.9/install_config/install/host_preparation.html[prepared] and meets the https://docs.openshift.com/container-platform/3.9/install_config/install/prerequisites.html[minimum requirements], then the installation should complete without failure. 

Don't forget to create a user account after the installation completes:

----
# htpasswd /etc/origin/master/htpasswd <username>
----

Hopefully, you've found this post to be a useful aide in migrating from the OCP Quick Installation method to using Ansible proper. Stay tuned for upcoming blogs from the Red Hat Connect team.
